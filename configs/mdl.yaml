# MDLM model configuration
defaults:
  - training

model_cfg:
  vocab_size: 50257
  n_ctx: 1024
  d_model: 768
  num_heads: 6
  n_layers: 12
  alpha_type: "cosine"
  alpha_min: 0.001
  alpha_max: 1.0
  continuous_time: true
  use_subs_parameterization: true
  num_sampling_steps: 100
  use_low_discrepancy_sampler: true
  sar_chunk_size: 512

# Override training parameters if needed
train_seq_len: 1024
num_iterations: 1750