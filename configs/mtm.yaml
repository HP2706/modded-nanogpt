# MultiTokenModel configuration
defaults:
  - training

model_cfg:
  vocab_size: 50257
  n_ctx: 1024
  d_model: 768
  num_heads: 6
  n_layers: 12
  n_mtp_modules: 2
  mtp_lambda: 0.1

# Override training parameters if needed
train_seq_len: 1024
num_iterations: 1750