# MTP model configuration
defaults:
  - training

model_cfg:
  vocab_size: 50257
  num_layers: 12
  num_heads: 6
  model_dim: 768
  n_mtp_tokens: 2
  use_liger: true
  use_deepseek_mtp: false
  proj_fp8: false

# Override training parameters if needed
train_seq_len: 49152
num_iterations: 1750