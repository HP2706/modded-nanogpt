numpy
tqdm
torch
huggingface-hub
liger-kernel
wandb<=0.18.7
transformers
adam-mini
transformers
tqdm
hf-transfer
fire
native-sparse-attention-pytorch # https://github.com/lucidrains/native-sparse-attention-pytorch
einx>=0.3.0
einops>=0.8.1
jaxtyping
matplotlib
flash-linear-attention @ git+https://github.com/fla-org/flash-linear-attention.git